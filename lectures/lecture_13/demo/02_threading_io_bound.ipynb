{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Threading & I/O-Bound Tasks\n",
    "\n",
    "## Topic 2: When Waiting is the Bottleneck\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "1. Why threading works for I/O-bound tasks\n",
    "2. `ThreadPoolExecutor` basics\n",
    "3. Two patterns: `map()` vs `submit()` + `as_completed()`\n",
    "4. Exception handling in threads\n",
    "5. Practical file processing example\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Number of CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: I/O-Bound vs CPU-Bound Demo\n",
    "\n",
    "Let's first understand the difference between I/O-bound and CPU-bound tasks, and see why threading helps with I/O but not CPU work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an I/O-bound task (like fetching data from an API)\n",
    "def io_bound_task(task_id):\n",
    "    \"\"\"Simulate network I/O - waiting for data.\"\"\"\n",
    "    time.sleep(0.3)  # Simulate network delay\n",
    "    return f\"Task {task_id} completed\"\n",
    "\n",
    "# Simulate a CPU-bound task (like heavy computation)\n",
    "def cpu_bound_task(task_id):\n",
    "    \"\"\"Simulate CPU-intensive work.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(2_000_000):\n",
    "        total += i * i\n",
    "    return f\"Task {task_id}: {total}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test I/O-bound: Sequential vs Threaded\n",
    "n_tasks = 8\n",
    "\n",
    "print(\"=== I/O-BOUND TASKS ===\")\n",
    "print(f\"Running {n_tasks} tasks that each wait 0.3 seconds...\\n\")\n",
    "\n",
    "# Sequential\n",
    "start = time.time()\n",
    "results = [io_bound_task(i) for i in range(n_tasks)]\n",
    "seq_time = time.time() - start\n",
    "print(f\"Sequential: {seq_time:.2f}s\")\n",
    "\n",
    "# Threaded\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    results = list(executor.map(io_bound_task, range(n_tasks)))\n",
    "thread_time = time.time() - start\n",
    "print(f\"Threaded:   {thread_time:.2f}s\")\n",
    "print(f\"Speedup:    {seq_time/thread_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CPU-bound: Sequential vs Threaded\n",
    "n_tasks = 4\n",
    "\n",
    "print(\"\\n=== CPU-BOUND TASKS ===\")\n",
    "print(f\"Running {n_tasks} CPU-intensive tasks...\\n\")\n",
    "\n",
    "# Sequential\n",
    "start = time.time()\n",
    "results = [cpu_bound_task(i) for i in range(n_tasks)]\n",
    "seq_time = time.time() - start\n",
    "print(f\"Sequential: {seq_time:.2f}s\")\n",
    "\n",
    "# Threaded\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    results = list(executor.map(cpu_bound_task, range(n_tasks)))\n",
    "thread_time = time.time() - start\n",
    "print(f\"Threaded:   {thread_time:.2f}s\")\n",
    "print(f\"Speedup:    {seq_time/thread_time:.1f}x (no speedup due to GIL!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observation\n",
    "\n",
    "- **I/O-bound tasks**: Threading gives near-perfect speedup (8 tasks, 8x faster)\n",
    "- **CPU-bound tasks**: Threading gives NO speedup (or even slower!) due to the GIL\n",
    "\n",
    "**Remember**: Use `ThreadPoolExecutor` for I/O, `ProcessPoolExecutor` for CPU work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Pattern 1 - executor.map()\n",
    "\n",
    "The simplest pattern: apply the same function to many inputs.\n",
    "\n",
    "### Finance Example: Fetching Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fetch_stock_data(ticker):\n",
    "    \"\"\"\n",
    "    Simulate fetching stock data from an API.\n",
    "    In real life, this would be a call to Yahoo Finance, Alpha Vantage, etc.\n",
    "    \"\"\"\n",
    "    # Simulate variable network latency (100-500ms)\n",
    "    delay = 0.1 + 0.4 * np.random.random()\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    # Generate fake stock data\n",
    "    np.random.seed(hash(ticker) % 2**32)\n",
    "    base_price = np.random.uniform(10, 500)\n",
    "    \n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'price': round(base_price, 2),\n",
    "        'change': round(np.random.uniform(-5, 5), 2),\n",
    "        'volume': np.random.randint(100000, 10000000),\n",
    "        'fetch_time': round(delay, 3)\n",
    "    }\n",
    "\n",
    "# Test with one ticker\n",
    "print(simulate_fetch_stock_data('AAPL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio of tickers to fetch\n",
    "tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META', 'NVDA', 'TSLA', 'JPM', \n",
    "           'BAC', 'WMT', 'PG', 'JNJ', 'UNH', 'HD', 'V', 'MA']\n",
    "\n",
    "print(f\"Fetching data for {len(tickers)} stocks...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential fetching\n",
    "print(\"Sequential fetching:\")\n",
    "start = time.time()\n",
    "sequential_results = [simulate_fetch_stock_data(t) for t in tickers]\n",
    "seq_time = time.time() - start\n",
    "print(f\"Time: {seq_time:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel fetching with executor.map()\n",
    "print(\"Parallel fetching with ThreadPoolExecutor:\")\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    parallel_results = list(executor.map(simulate_fetch_stock_data, tickers))\n",
    "par_time = time.time() - start\n",
    "print(f\"Time: {par_time:.2f}s\")\n",
    "print(f\"Speedup: {seq_time/par_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results come back in INPUT ORDER (same as tickers list)\n",
    "df = pd.DataFrame(parallel_results)\n",
    "print(\"\\nResults (note: order matches input ticker list):\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use `executor.map()`\n",
    "\n",
    "- Same function applied to many inputs\n",
    "- You need results in the **same order** as inputs\n",
    "- Simple, clean code is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Pattern 2 - submit() + as_completed()\n",
    "\n",
    "More flexible pattern: process results as they arrive.\n",
    "\n",
    "### When is this useful?\n",
    "- Show progress to the user\n",
    "- Process fast results while waiting for slow ones\n",
    "- Early termination when you find what you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_with_variable_delay(ticker):\n",
    "    \"\"\"\n",
    "    Simulate fetching where some tickers are fast and some are slow.\n",
    "    \"\"\"\n",
    "    # Some tickers are \"slow\" (like international exchanges)\n",
    "    slow_tickers = {'TSLA', 'META', 'NVDA'}\n",
    "    delay = 0.8 if ticker in slow_tickers else 0.2\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    np.random.seed(hash(ticker) % 2**32)\n",
    "    return {\n",
    "        'ticker': ticker,\n",
    "        'price': round(np.random.uniform(10, 500), 2),\n",
    "        'delay': delay\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using submit() + as_completed() to process results as they arrive\n",
    "tickers = ['AAPL', 'TSLA', 'GOOGL', 'META', 'MSFT', 'NVDA', 'AMZN', 'JPM']\n",
    "\n",
    "print(\"Processing results as they complete:\\n\")\n",
    "\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {executor.submit(fetch_with_variable_delay, t): t for t in tickers}\n",
    "    \n",
    "    # Process results as they complete (NOT in submission order!)\n",
    "    results = []\n",
    "    for future in as_completed(futures):\n",
    "        ticker = futures[future]\n",
    "        result = future.result()\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{elapsed:.2f}s] Got {ticker}: ${result['price']} (took {result['delay']}s)\")\n",
    "        results.append(result)\n",
    "\n",
    "total_time = time.time() - start\n",
    "print(f\"\\nTotal time: {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice!\n",
    "\n",
    "- Fast tickers (AAPL, GOOGL, MSFT, AMZN, JPM) complete first\n",
    "- Slow tickers (TSLA, META, NVDA) complete last\n",
    "- Results arrive in **completion order**, not submission order\n",
    "\n",
    "This is useful for:\n",
    "- Showing a progress bar\n",
    "- Processing available data while waiting for the rest\n",
    "- Canceling slow tasks if you've found what you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Exception Handling\n",
    "\n",
    "In real applications, some operations will fail. Here's how to handle errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unreliable_fetch(ticker):\n",
    "    \"\"\"\n",
    "    Simulate an unreliable API that sometimes fails.\n",
    "    \"\"\"\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    # Simulate failures for certain tickers\n",
    "    if ticker in ['BAD', 'INVALID', 'ERROR']:\n",
    "        raise ValueError(f\"Invalid ticker symbol: {ticker}\")\n",
    "    \n",
    "    # Simulate random network failures (10% chance)\n",
    "    if np.random.random() < 0.1:\n",
    "        raise ConnectionError(f\"Network timeout for {ticker}\")\n",
    "    \n",
    "    np.random.seed(hash(ticker) % 2**32)\n",
    "    return {'ticker': ticker, 'price': round(np.random.uniform(10, 500), 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of valid and invalid tickers\n",
    "tickers = ['AAPL', 'BAD', 'GOOGL', 'INVALID', 'MSFT', 'AMZN']\n",
    "\n",
    "print(\"Fetching with error handling:\\n\")\n",
    "\n",
    "successful = []\n",
    "failed = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = {executor.submit(unreliable_fetch, t): t for t in tickers}\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        ticker = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            print(f\"  SUCCESS: {ticker} = ${result['price']}\")\n",
    "            successful.append(result)\n",
    "        except ValueError as e:\n",
    "            print(f\"  FAILED (invalid): {ticker} - {e}\")\n",
    "            failed.append({'ticker': ticker, 'error': str(e)})\n",
    "        except ConnectionError as e:\n",
    "            print(f\"  FAILED (network): {ticker} - {e}\")\n",
    "            failed.append({'ticker': ticker, 'error': str(e)})\n",
    "\n",
    "print(f\"\\nSuccessful: {len(successful)}, Failed: {len(failed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice: Always Handle Exceptions\n",
    "\n",
    "When using `future.result()`, always wrap it in try/except:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    result = future.result()\n",
    "except Exception as e:\n",
    "    # Log the error, retry, or mark as failed\n",
    "    print(f\"Task failed: {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Practical Example - Parallel File Processing\n",
    "\n",
    "A common finance task: processing multiple data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's generate some sample files\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary directory for our sample files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Created temp directory: {temp_dir}\\n\")\n",
    "\n",
    "# Generate sample stock data files\n",
    "stocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META', 'NVDA', 'TSLA', 'JPM']\n",
    "dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')\n",
    "\n",
    "print(\"Generating sample data files...\")\n",
    "for ticker in stocks:\n",
    "    np.random.seed(hash(ticker) % 2**32)\n",
    "    \n",
    "    # Generate random price data\n",
    "    returns = np.random.normal(0.0005, 0.02, len(dates))\n",
    "    prices = 100 * np.cumprod(1 + returns)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'close': prices,\n",
    "        'volume': np.random.randint(1000000, 10000000, len(dates))\n",
    "    })\n",
    "    \n",
    "    filepath = os.path.join(temp_dir, f\"{ticker}.csv\")\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"  Created {ticker}.csv ({len(df)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stock_file(filepath):\n",
    "    \"\"\"\n",
    "    Read a stock file and calculate summary statistics.\n",
    "    This simulates I/O (file reading) followed by light computation.\n",
    "    \"\"\"\n",
    "    # Simulate some I/O delay (file reading from slow storage)\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    ticker = os.path.basename(filepath).replace('.csv', '')\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'ticker': ticker,\n",
    "        'start_price': df['close'].iloc[0],\n",
    "        'end_price': df['close'].iloc[-1],\n",
    "        'total_return': (df['close'].iloc[-1] / df['close'].iloc[0] - 1) * 100,\n",
    "        'volatility': df['returns'].std() * np.sqrt(252) * 100,  # Annualized\n",
    "        'avg_volume': df['volume'].mean(),\n",
    "        'sharpe': (df['returns'].mean() * 252) / (df['returns'].std() * np.sqrt(252))\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all CSV files\n",
    "csv_files = [os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.csv')]\n",
    "print(f\"Processing {len(csv_files)} files...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential processing\n",
    "print(\"Sequential processing:\")\n",
    "start = time.time()\n",
    "sequential_stats = [process_stock_file(f) for f in csv_files]\n",
    "seq_time = time.time() - start\n",
    "print(f\"Time: {seq_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing\n",
    "print(\"\\nParallel processing:\")\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    parallel_stats = list(executor.map(process_stock_file, csv_files))\n",
    "par_time = time.time() - start\n",
    "print(f\"Time: {par_time:.2f}s\")\n",
    "print(f\"Speedup: {seq_time/par_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(parallel_stats)\n",
    "results_df = results_df.round(2)\n",
    "print(\"\\nStock Summary Statistics:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temp files\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Cleaned up temp directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Threading for I/O-Bound Tasks\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Threading works for I/O-bound tasks** because the GIL is released during I/O operations\n",
    "\n",
    "2. **Two main patterns**:\n",
    "   - `executor.map(func, items)` - Simple, results in order\n",
    "   - `executor.submit()` + `as_completed()` - Process results as they arrive\n",
    "\n",
    "3. **Always handle exceptions** when calling `future.result()`\n",
    "\n",
    "4. **Don't use threading for CPU-bound work** - you won't get speedup due to the GIL\n",
    "\n",
    "### When to Use Threading\n",
    "\n",
    "| Use Threading For | Don't Use Threading For |\n",
    "|-------------------|------------------------|\n",
    "| API calls | Monte Carlo simulations |\n",
    "| Database queries | Heavy calculations |\n",
    "| File I/O | Number crunching |\n",
    "| Network requests | Optimization algorithms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Parallel Data Fetching\n",
    "\n",
    "Modify the `simulate_fetch_stock_data` function to also return the 52-week high and low. Then fetch data for 20 tickers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def fetch_extended_data(ticker):\n",
    "    \"\"\"Fetch extended stock data including 52-week high/low.\"\"\"\n",
    "    pass  # Implement me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Retry Logic\n",
    "\n",
    "Implement a wrapper function that retries failed fetches up to 3 times before giving up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def fetch_with_retry(ticker, max_retries=3):\n",
    "    \"\"\"Fetch data with automatic retry on failure.\"\"\"\n",
    "    pass  # Implement me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Progress Tracking\n",
    "\n",
    "Using `tqdm`, add a progress bar to the parallel file processing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: Use tqdm with as_completed\n",
    "# from tqdm import tqdm\n",
    "# for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "#     ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
