{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Real-World Finance Applications\n",
    "\n",
    "## Putting It All Together\n",
    "\n",
    "In this notebook, we'll build complete, practical finance applications:\n",
    "1. **Parallel Backtesting Engine** - Test trading strategies across parameter grids\n",
    "2. **Bootstrap Confidence Intervals** - Statistical inference for Sharpe ratios\n",
    "3. **Rolling Correlation Analysis** - Multi-asset correlation computation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "n_cores = os.cpu_count()\n",
    "print(f\"Available CPU cores: {n_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Application 1: Parallel Backtesting Engine\n",
    "\n",
    "### The Problem\n",
    "\n",
    "You want to test a moving average crossover strategy with different parameter combinations:\n",
    "- Short window: 5-50 days\n",
    "- Long window: 20-200 days\n",
    "\n",
    "That's potentially hundreds of combinations to test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic stock price data\n",
    "def generate_stock_data(n_days=2520, seed=42):  # 10 years of daily data\n",
    "    \"\"\"Generate synthetic stock price data with realistic properties.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Parameters for a typical stock\n",
    "    mu = 0.08 / 252  # Daily expected return (8% annual)\n",
    "    sigma = 0.20 / np.sqrt(252)  # Daily volatility (20% annual)\n",
    "    \n",
    "    # Generate returns with some autocorrelation and fat tails\n",
    "    returns = np.random.standard_t(df=5, size=n_days) * sigma + mu\n",
    "    \n",
    "    # Convert to prices\n",
    "    prices = 100 * np.cumprod(1 + returns)\n",
    "    \n",
    "    dates = pd.date_range('2014-01-01', periods=n_days, freq='D')\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'close': prices\n",
    "    }).set_index('date')\n",
    "\n",
    "# Generate data\n",
    "stock_data = generate_stock_data()\n",
    "print(f\"Generated {len(stock_data)} days of price data\")\n",
    "print(f\"Date range: {stock_data.index[0].date()} to {stock_data.index[-1].date()}\")\n",
    "print(f\"Price range: ${stock_data['close'].min():.2f} to ${stock_data['close'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the price data\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(stock_data.index, stock_data['close'], linewidth=0.8)\n",
    "plt.title('Synthetic Stock Price Data (10 Years)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_ma_crossover(args):\n",
    "    \"\"\"\n",
    "    Backtest a moving average crossover strategy.\n",
    "    \n",
    "    Strategy:\n",
    "    - Go long when short MA crosses above long MA\n",
    "    - Go flat when short MA crosses below long MA\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    args : tuple of (prices, short_window, long_window)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with strategy performance metrics\n",
    "    \"\"\"\n",
    "    prices, short_window, long_window = args\n",
    "    \n",
    "    # Skip invalid combinations\n",
    "    if short_window >= long_window:\n",
    "        return None\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    short_ma = prices.rolling(window=short_window).mean()\n",
    "    long_ma = prices.rolling(window=long_window).mean()\n",
    "    \n",
    "    # Generate signals: 1 = long, 0 = flat\n",
    "    signal = (short_ma > long_ma).astype(int)\n",
    "    signal = signal.shift(1)  # Trade on next day's open\n",
    "    \n",
    "    # Calculate returns\n",
    "    daily_returns = prices.pct_change()\n",
    "    strategy_returns = signal * daily_returns\n",
    "    \n",
    "    # Remove NaN values\n",
    "    strategy_returns = strategy_returns.dropna()\n",
    "    \n",
    "    if len(strategy_returns) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_return = (1 + strategy_returns).prod() - 1\n",
    "    annual_return = (1 + total_return) ** (252 / len(strategy_returns)) - 1\n",
    "    volatility = strategy_returns.std() * np.sqrt(252)\n",
    "    sharpe = annual_return / volatility if volatility > 0 else 0\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative = (1 + strategy_returns).cumprod()\n",
    "    rolling_max = cumulative.cummax()\n",
    "    drawdown = (cumulative - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Number of trades\n",
    "    trades = signal.diff().abs().sum() / 2\n",
    "    \n",
    "    return {\n",
    "        'short_window': short_window,\n",
    "        'long_window': long_window,\n",
    "        'total_return': total_return * 100,\n",
    "        'annual_return': annual_return * 100,\n",
    "        'volatility': volatility * 100,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'max_drawdown': max_drawdown * 100,\n",
    "        'n_trades': trades\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "short_windows = range(5, 51, 5)    # 5, 10, 15, ..., 50\n",
    "long_windows = range(20, 201, 10)  # 20, 30, 40, ..., 200\n",
    "\n",
    "# Create all combinations\n",
    "param_combinations = list(product(short_windows, long_windows))\n",
    "print(f\"Testing {len(param_combinations)} parameter combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare arguments for parallel execution\n",
    "prices = stock_data['close']\n",
    "backtest_args = [(prices, short, long) for short, long in param_combinations]\n",
    "\n",
    "# Sequential backtesting\n",
    "print(\"Sequential backtesting:\")\n",
    "start = time.time()\n",
    "sequential_results = [backtest_ma_crossover(args) for args in backtest_args]\n",
    "sequential_results = [r for r in sequential_results if r is not None]\n",
    "seq_time = time.time() - start\n",
    "print(f\"  Time: {seq_time:.2f}s\")\n",
    "print(f\"  Valid combinations: {len(sequential_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel backtesting\n",
    "print(\"\\nParallel backtesting:\")\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "    parallel_results = list(executor.map(backtest_ma_crossover, backtest_args))\n",
    "parallel_results = [r for r in parallel_results if r is not None]\n",
    "par_time = time.time() - start\n",
    "print(f\"  Time: {par_time:.2f}s\")\n",
    "print(f\"  Speedup: {seq_time/par_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "results_df = pd.DataFrame(parallel_results)\n",
    "\n",
    "print(\"\\nTop 10 Strategies by Sharpe Ratio:\")\n",
    "print(results_df.nlargest(10, 'sharpe_ratio')[[\n",
    "    'short_window', 'long_window', 'annual_return', 'sharpe_ratio', 'max_drawdown'\n",
    "]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of Sharpe ratios\n",
    "pivot = results_df.pivot(index='short_window', columns='long_window', values='sharpe_ratio')\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.imshow(pivot.values, aspect='auto', cmap='RdYlGn', origin='lower')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "\n",
    "# Set tick labels\n",
    "plt.xticks(range(len(pivot.columns))[::2], pivot.columns[::2])\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "\n",
    "plt.xlabel('Long Window (days)')\n",
    "plt.ylabel('Short Window (days)')\n",
    "plt.title('Strategy Performance Heatmap (Sharpe Ratio)\\nMoving Average Crossover Strategy')\n",
    "\n",
    "# Mark best strategy\n",
    "best = results_df.loc[results_df['sharpe_ratio'].idxmax()]\n",
    "best_short_idx = list(pivot.index).index(best['short_window'])\n",
    "best_long_idx = list(pivot.columns).index(best['long_window'])\n",
    "plt.plot(best_long_idx, best_short_idx, 'k*', markersize=15, label=f\"Best: {int(best['short_window'])}/{int(best['long_window'])}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest Strategy: Short={int(best['short_window'])}, Long={int(best['long_window'])}\")\n",
    "print(f\"  Annual Return: {best['annual_return']:.1f}%\")\n",
    "print(f\"  Sharpe Ratio: {best['sharpe_ratio']:.2f}\")\n",
    "print(f\"  Max Drawdown: {best['max_drawdown']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Application 2: Bootstrap Confidence Intervals\n",
    "\n",
    "### The Problem\n",
    "\n",
    "You've calculated a Sharpe ratio of 1.5. But how confident are you in this estimate?\n",
    "\n",
    "Bootstrap resampling gives us confidence intervals without assuming normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample returns data\n",
    "np.random.seed(42)\n",
    "n_days = 756  # 3 years of daily returns\n",
    "\n",
    "# Simulate returns with realistic properties\n",
    "daily_returns = np.random.standard_t(df=5, size=n_days) * 0.015 + 0.0003\n",
    "\n",
    "# Calculate actual Sharpe ratio\n",
    "actual_sharpe = (np.mean(daily_returns) * 252) / (np.std(daily_returns) * np.sqrt(252))\n",
    "print(f\"Sample size: {n_days} days\")\n",
    "print(f\"Observed Sharpe Ratio: {actual_sharpe:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe(returns):\n",
    "    \"\"\"Calculate annualized Sharpe ratio.\"\"\"\n",
    "    return (np.mean(returns) * 252) / (np.std(returns) * np.sqrt(252))\n",
    "\n",
    "def bootstrap_sharpe_batch(args):\n",
    "    \"\"\"\n",
    "    Generate bootstrap samples and calculate Sharpe ratios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    args : tuple of (returns, n_samples, seed)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array of bootstrap Sharpe ratios\n",
    "    \"\"\"\n",
    "    returns, n_samples, seed = args\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n = len(returns)\n",
    "    bootstrap_sharpes = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Resample with replacement\n",
    "        sample_idx = np.random.randint(0, n, size=n)\n",
    "        sample_returns = returns[sample_idx]\n",
    "        bootstrap_sharpes[i] = calculate_sharpe(sample_returns)\n",
    "    \n",
    "    return bootstrap_sharpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap parameters\n",
    "total_samples = 10_000\n",
    "n_batches = 8\n",
    "samples_per_batch = total_samples // n_batches\n",
    "\n",
    "print(f\"Total bootstrap samples: {total_samples:,}\")\n",
    "print(f\"Batches: {n_batches}\")\n",
    "\n",
    "# Prepare batch arguments\n",
    "batch_args = [\n",
    "    (daily_returns, samples_per_batch, seed)\n",
    "    for seed in range(n_batches)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential bootstrap\n",
    "print(\"Sequential bootstrap:\")\n",
    "start = time.time()\n",
    "seq_sharpes = [bootstrap_sharpe_batch(args) for args in batch_args]\n",
    "seq_sharpes = np.concatenate(seq_sharpes)\n",
    "seq_time = time.time() - start\n",
    "print(f\"  Time: {seq_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel bootstrap\n",
    "print(\"\\nParallel bootstrap:\")\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "    par_sharpes = list(executor.map(bootstrap_sharpe_batch, batch_args))\n",
    "par_sharpes = np.concatenate(par_sharpes)\n",
    "par_time = time.time() - start\n",
    "print(f\"  Time: {par_time:.2f}s\")\n",
    "print(f\"  Speedup: {seq_time/par_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence intervals\n",
    "ci_90 = np.percentile(par_sharpes, [5, 95])\n",
    "ci_95 = np.percentile(par_sharpes, [2.5, 97.5])\n",
    "ci_99 = np.percentile(par_sharpes, [0.5, 99.5])\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"BOOTSTRAP RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Observed Sharpe Ratio: {actual_sharpe:.3f}\")\n",
    "print(f\"Bootstrap Mean: {np.mean(par_sharpes):.3f}\")\n",
    "print(f\"Bootstrap Std: {np.std(par_sharpes):.3f}\")\n",
    "print(f\"\\nConfidence Intervals:\")\n",
    "print(f\"  90% CI: [{ci_90[0]:.3f}, {ci_90[1]:.3f}]\")\n",
    "print(f\"  95% CI: [{ci_95[0]:.3f}, {ci_95[1]:.3f}]\")\n",
    "print(f\"  99% CI: [{ci_99[0]:.3f}, {ci_99[1]:.3f}]\")\n",
    "print(f\"\\nProbability Sharpe > 0: {(par_sharpes > 0).mean()*100:.1f}%\")\n",
    "print(f\"Probability Sharpe > 1: {(par_sharpes > 1).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bootstrap distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(par_sharpes, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "\n",
    "# Add vertical lines for CIs and observed value\n",
    "ax.axvline(actual_sharpe, color='red', linewidth=2, linestyle='-', label=f'Observed: {actual_sharpe:.3f}')\n",
    "ax.axvline(ci_95[0], color='orange', linewidth=2, linestyle='--', label=f'95% CI: [{ci_95[0]:.2f}, {ci_95[1]:.2f}]')\n",
    "ax.axvline(ci_95[1], color='orange', linewidth=2, linestyle='--')\n",
    "ax.axvline(0, color='black', linewidth=1, linestyle=':')\n",
    "\n",
    "ax.set_xlabel('Sharpe Ratio')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Bootstrap Distribution of Sharpe Ratio')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Application 3: Rolling Correlation Analysis\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Calculate rolling correlations for a multi-asset portfolio. With many assets, this becomes computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-asset return data\n",
    "np.random.seed(42)\n",
    "n_days = 2520  # 10 years\n",
    "n_assets = 20\n",
    "\n",
    "# Asset names\n",
    "assets = [f'Asset_{i+1:02d}' for i in range(n_assets)]\n",
    "\n",
    "# Generate correlated returns\n",
    "# Create a random correlation matrix\n",
    "base_corr = np.random.uniform(0.2, 0.6, (n_assets, n_assets))\n",
    "base_corr = (base_corr + base_corr.T) / 2  # Make symmetric\n",
    "np.fill_diagonal(base_corr, 1.0)\n",
    "\n",
    "# Ensure positive semi-definite\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(base_corr)\n",
    "eigenvalues = np.maximum(eigenvalues, 0.01)\n",
    "corr_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
    "\n",
    "# Cholesky decomposition\n",
    "L = np.linalg.cholesky(corr_matrix)\n",
    "\n",
    "# Generate correlated returns\n",
    "volatilities = np.random.uniform(0.15, 0.35, n_assets) / np.sqrt(252)\n",
    "means = np.random.uniform(0.05, 0.15, n_assets) / 252\n",
    "\n",
    "uncorrelated = np.random.standard_normal((n_days, n_assets))\n",
    "correlated = uncorrelated @ L.T\n",
    "returns = correlated * volatilities + means\n",
    "\n",
    "returns_df = pd.DataFrame(returns, columns=assets)\n",
    "\n",
    "print(f\"Generated returns for {n_assets} assets over {n_days} days\")\n",
    "print(f\"\\nShape: {returns_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_corr_for_window(args):\n",
    "    \"\"\"\n",
    "    Calculate correlation matrix for a specific time window.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    args : tuple of (returns_array, start_idx, window_size)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (start_idx, correlation_matrix)\n",
    "    \"\"\"\n",
    "    returns_array, start_idx, window_size = args\n",
    "    \n",
    "    window_returns = returns_array[start_idx:start_idx + window_size]\n",
    "    corr_matrix = np.corrcoef(window_returns.T)\n",
    "    \n",
    "    return start_idx, corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for rolling correlation\n",
    "window_size = 63  # Quarterly rolling window\n",
    "step_size = 21    # Calculate every month\n",
    "\n",
    "# Create list of window start indices\n",
    "start_indices = list(range(0, n_days - window_size, step_size))\n",
    "print(f\"Number of windows: {len(start_indices)}\")\n",
    "print(f\"Correlation matrices to compute: {len(start_indices)}\")\n",
    "\n",
    "# Prepare arguments\n",
    "returns_array = returns_df.values\n",
    "corr_args = [(returns_array, idx, window_size) for idx in start_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential calculation\n",
    "print(\"Sequential calculation:\")\n",
    "start = time.time()\n",
    "seq_results = [calculate_rolling_corr_for_window(args) for args in corr_args]\n",
    "seq_time = time.time() - start\n",
    "print(f\"  Time: {seq_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel calculation\n",
    "print(\"\\nParallel calculation:\")\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=n_cores) as executor:\n",
    "    par_results = list(executor.map(calculate_rolling_corr_for_window, corr_args))\n",
    "par_time = time.time() - start\n",
    "print(f\"  Time: {par_time:.3f}s\")\n",
    "print(f\"  Speedup: {seq_time/par_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract average correlation over time\n",
    "avg_correlations = []\n",
    "for idx, corr_matrix in par_results:\n",
    "    # Get upper triangle (excluding diagonal)\n",
    "    upper_tri = corr_matrix[np.triu_indices(n_assets, k=1)]\n",
    "    avg_correlations.append({\n",
    "        'window_start': idx,\n",
    "        'avg_correlation': np.mean(upper_tri),\n",
    "        'min_correlation': np.min(upper_tri),\n",
    "        'max_correlation': np.max(upper_tri)\n",
    "    })\n",
    "\n",
    "corr_df = pd.DataFrame(avg_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Average correlation over time\n",
    "ax1 = axes[0]\n",
    "ax1.plot(corr_df['window_start'], corr_df['avg_correlation'], 'b-', linewidth=1.5, label='Average')\n",
    "ax1.fill_between(corr_df['window_start'], corr_df['min_correlation'], corr_df['max_correlation'], \n",
    "                  alpha=0.3, color='blue', label='Min-Max Range')\n",
    "ax1.set_xlabel('Trading Day')\n",
    "ax1.set_ylabel('Correlation')\n",
    "ax1.set_title('Rolling Pairwise Correlations Over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Final correlation heatmap\n",
    "ax2 = axes[1]\n",
    "final_corr = par_results[-1][1]\n",
    "im = ax2.imshow(final_corr, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "plt.colorbar(im, ax=ax2, label='Correlation')\n",
    "ax2.set_xticks(range(0, n_assets, 2))\n",
    "ax2.set_yticks(range(0, n_assets, 2))\n",
    "ax2.set_xticklabels([assets[i] for i in range(0, n_assets, 2)], rotation=45, ha='right')\n",
    "ax2.set_yticklabels([assets[i] for i in range(0, n_assets, 2)])\n",
    "ax2.set_title('Final Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "1. **Parallel Backtesting Engine**\n",
    "   - Tested 190 strategy parameter combinations\n",
    "   - Found optimal moving average parameters\n",
    "   - Visualized performance across parameter space\n",
    "\n",
    "2. **Bootstrap Confidence Intervals**\n",
    "   - Generated 10,000 bootstrap samples\n",
    "   - Calculated 95% confidence interval for Sharpe ratio\n",
    "   - Properly quantified uncertainty in our estimate\n",
    "\n",
    "3. **Rolling Correlation Analysis**\n",
    "   - Computed correlation matrices for 100+ time windows\n",
    "   - Tracked average correlation over time\n",
    "   - Visualized correlation dynamics\n",
    "\n",
    "### Key Patterns Used\n",
    "\n",
    "| Application | Pattern | Why |\n",
    "|-------------|---------|-----|\n",
    "| Backtesting | `ProcessPoolExecutor.map()` | CPU-bound, many independent tasks |\n",
    "| Bootstrap | Batch processing with seeds | Control randomness, reduce overhead |\n",
    "| Correlations | Map over time windows | Each window is independent |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
